<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-17T09:51:21+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Arte Moderni 기술블로그</title><subtitle>우리 FIS와 글로벌소프트웨어 캠퍼스가 함께하는 &quot;우리 FISA - 클라우드 엔지니어링&quot; 파이널 프로젝트 6팀의 기술블로그입니다. 기술 지식에 대한 탐구와 학습 그리고 프로젝트 진행상황을 확인하실 수 있습니다.</subtitle><author><name>클라우드 엔지니어링 6팀</name></author><entry><title type="html">프로젝트 대체 어떻게 하는걸까?</title><link href="http://localhost:4000/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2023/08/15/post03.html" rel="alternate" type="text/html" title="프로젝트 대체 어떻게 하는걸까?" /><published>2023-08-15T00:00:00+09:00</published><updated>2023-08-15T00:00:00+09:00</updated><id>http://localhost:4000/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2023/08/15/post03</id><content type="html" xml:base="http://localhost:4000/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2023/08/15/post03.html"><![CDATA[<h1 id="프로젝트를-진행하는-방법">프로젝트를 진행하는 방법</h1>

<p>안녕하세요 F4팀의 PM 김지운입니다.<br />
<br />
빠르게 지나간 교육 시간을 지나 8월에 들어 프로젝트를 시작하게 되었는데요. 이전에 경험했던 부트캠프에서는 “애자일”과 “스크럼” 프레임워크를 교육받고 학습하며 프로젝트에 적용시켰습니다.<br />
하지만, 이번 우리 FISA 1기에서는 워터폴 방법을 적용하여 프로젝트가 진행되고 있는데요. 정보처리기사 시험에서만 봐온 단어인 “애자일”, “폭포수(워터폴)” 모형. 과연 어떻게 적용되는걸까요.
<br /><br />
간혹 애자일(Agile) 방법이 워터폴(Waterfall) 방법론 보다 더 많은 장점을 가지고 있는것처럼 설명되기도 합니다. 애자일 방법론을 적용하면 빠르게 변화하는 IT산업에서 빠르게 대응할 수 있기 때문이라고 생각합니다. 하지만, 실상은 조금 다른데요.
<br /><br />
알고 지내는 개발자분이나 우리 FISA를 통해 뵙게된 현직자분들과 특강을 통해 알게 된 사실은 “현실은 애자일 방법론을 따르기 어렵다”는 점입니다.
<br /><br />
회사를 크게 IT, 비IT 기업으로 구분짓는다고 해도, 비즈니스를 위해선 따라야 하는 절차가 있습니다. 그 절차를 위해 준비해야할 서류들은 물론이고, 미래를 내다보아야 하는 설계가 필요하게 되는것이죠. 따라서, 애자일 방법론을 적용시키려고 노력은 하고 싶으나 사업을 진행하기 위해선 서류상으로 오고가야할 “준비된 정보”가 중요해진다는 얘기입니다.</p>

<h2 id="agile-vs-waterfall">Agile VS Waterfall</h2>

<p><img src="https://github.com/Jimoou/Coding-Test/assets/109801772/d539dcf8-0a4a-4d3b-8f4d-fa60e704e8a9" alt="7456408" />
이미지 출처: freepik
<br /><br />
이번에 진행되는 프로젝트는 “워터폴”방법을 따르고 있습니다. 현업에서처럼 [요구사항 정의서 - 시스템 아키텍처 설계도 - WBS - DB 명세서] 등등 프로젝트 초기에 이러한 산출물들이 구비가 되어야 하고, 또 평가 받는 과정이 진행됩니다.
<br /><br />
애자일이 워터폴에 비해 유연하다고 해서 반대의 관점을 가진 방법론은 아니라고 생각합니다.
가끔 애자일을 ‘즉흥적’이라는 시선으로 바라보기도 합니다.
<br /><br />
하지만, 애자일 방법론의 핵심은 “즉흥적”이라기보다는 “고객의 요구사항에 신속하게 대응하며, 변화를 수용하는 것”입니다. 다시 말해, 애자일은 변화의 빠른 페이스에 맞춰 프로젝트를 관리하는 방식입니다.
<br /><br />
그 반면, 워터폴 방식은 선형적이며 각 단계별로 철저하게 계획을 세워 전체적인 흐름을 파악합니다. 각 단계가 끝날 때까지 다음 단계로 넘어가지 않는 것이 원칙입니다. 이로 인해, 워터폴 방식은 초기 계획대로 프로젝트가 진행될 때 효과적입니다.
<br /><br />
하지만 현실에서는 모든 것이 계획대로 되지 않습니다. 새로운 요구사항이 중간에 생기거나, 예상치 못한 문제가 발생할 때, 애자일처럼 유연하게 대응하기는 어렵습니다.
<br /><br />
그렇다면, 어떤 방법론이 더 나은가? 그것은 “상황”에 따라 다릅니다.
<br /><br />
<strong>장기적인 프로젝트</strong>에서는 초기에 철저한 계획이 필요하다면, 워터폴이 적절할 수 있습니다. 반면, <strong>빠르게 변화하는 환경</strong>에서는 애자일 방식이 더 적합합니다.
<br /><br />
결론적으로, 애자일과 워터폴 둘 다 장단점이 있습니다. 어떤 프로젝트에 어떤 방법론을 적용할지 결정하는 것은 PM의 역할 중 하나입니다. 중요한 것은 두 방법론을 동시에 이해하고, 상황에 맞게 적절히 섞어 사용하는 능력입니다. 현실의 프로젝트 환경에서는 둘 중 하나만 고집하는 것이 아니라, 상황과 요구사항에 맞게 유연하게 접근하는 자세가 필요합니다.</p>

<h2 id="경험이-중요하다">경험이 중요하다.</h2>

<p><img src="https://github.com/Jimoou/Coding-Test/assets/109801772/a0919c35-ff2f-4ca3-997f-35ffc0eb7e89" alt="happy-young-asia-businessmen-businesswomen-meeting-brainstorming-ideas-about-new-paperwork-project-colleagues-working-together-planning-success-strategy-enjoy-teamwork-small-modern-office" />
이미지 출처: freepik
<br /><br />
애자일이나 워터폴이라는 방법론에 대한 이해는 이론만큼이나 실제로 프로젝트를 진행하면서 느낀 점이 중요하다고 생각합니다. 저의 첫 부트캠프 경험에서는 애자일 방법론이 강조되었지만, 사실상 스프린트 단위로 워터폴 방식을 따르는 듯한 느낌을 받았습니다.
<br /><br />
스프린트 계획을 세울 때마다, 단기간 내에 세부적인 계획을 세우고 실행하다보니 한 스프린트가 끝나기 전까지는 다음 단계로 나아가지 못하는 느낌을 받았습니다. 이는 워터폴의 선형적인 특성과 상당히 유사했습니다. 물론 스프린트 단위로 빠르게 반복되는 프로세스는 애자일의 특성을 살릴 수 있었지만, 그 안에서도 워터폴과 같은 접근 방식이 있었던 것입니다.
<br /><br />
이번 부트캠프에서는 순수하게 워터폴 방법론을 적용하고 있습니다. 프로젝트 초기부터 많은 서류와 계획을 세워야 하는 것이 힘들긴 했지만, 그로 인해 전체 프로젝트의 방향성과 목표가 명확해진 점은 확실하게 느꼈습니다.</p>]]></content><author><name>김지운</name></author><category term="프로젝트" /><category term="project" /><category term="agile" /><category term="process" /><summary type="html"><![CDATA[프로젝트를 진행하는 방법]]></summary></entry><entry><title type="html">Apache Kafka</title><link href="http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/15/post04.html" rel="alternate" type="text/html" title="Apache Kafka" /><published>2023-08-15T00:00:00+09:00</published><updated>2023-08-15T00:00:00+09:00</updated><id>http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/15/post04</id><content type="html" xml:base="http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/15/post04.html"><![CDATA[<p>안녕하세요 F4팀의 혁잔디입니다. Apache Kafka를 활용한 프로젝트를 기획하면서 Apache Kakfa에 대해 제가 이해한 점을 정리하고 공유하기 위해 글을 작성합니다. 부족한 부분이 있으면 말씀해주셔요😊
<br /><br /></p>
<h1 id="apache-kafka">Apache Kafka</h1>
<p>Apache Kafka는 웹사이트, 어플리케이션, 센서 등에 취합한 데이터를 스트림 파이프라인을 통해 실시간으로 관리하고 보내기 위한 분산 스트리밍 플랫폼입니다. 데이터를 생성하는 어플리케이션과 데이터를 소비하는 어플리케이션 간의 중재자 역할을 함으로써 데이터의 전송 제어, 처리, 관리 역할을 합니다. 카프카 시스템은 여러 요소(노드)와 함께 구성될 수 있어 카프카 클러스터 라고 하기도 하는데 다른 메시징 시스템과 마찬가지로 어플리케이션과 서버 간의 비동기 데이터 교환을 용이하게 하고, 하루에 <b>수 조개의</b> 이벤트 처리가 가능하게 하는 역할을 합니다. <br />
<b>즉, 카프카는 플랫폼에 서비스를 연결하여 다양한 서비스에서 나오는 데이터 흐름을 실시간으로 제어하는 서비스의 중추역할을 하는 플랫폼입니다.</b></p>
<h3 id="kafka의-주요-기능">Kafka의 주요 기능</h3>
<ol>
  <li>애플리케이션에서 데이터 또는 이벤트 스트림을 게시하거나 구독할 수 있게 합니다.</li>
  <li>장애가 발생하더라도 안전하고 안정적인 방식으로 정확하게 레코드를 저장합니다.</li>
  <li>레코드를 순서대로 실시간 처리합니다.</li>
</ol>

<h3 id="eventdriven이란">EventDriven이란?</h3>
<p><img src="https://camo.githubusercontent.com/c056c020adbf92b11a31e549c90d90ac7b8d319b519f61e733a474ded6a1fdcc/68747470733a2f2f706f737466696c65732e707374617469632e6e65742f4d6a41794d5441794d4452664d5467322f4d4441784e6a45794e44497a4d4451344e5467782e5058665f6c3530365331434d696b437933504c3456656a57347168636849362d6c69437151417765644645672e5f724841584a6a4644364272305442306e393979767a6d5a50314e76546c554b644c665856625457654f6f672e504e472e61726b646174612f312e706e673f747970653d77393636" alt="image" />
출처 : https://m.blog.naver.com/arkdata/222632637775<br />
<br />
Event Driven Architecture란 이벤트의 생산, 갑지, 소비 및 반응 또는 시스템 상태의 중대한 변화를 지원하는 소프트웨어 모델 또는 아키텍처의 패러다임이다. 분산 아키텍처 환경에서 이벤트를 생성하고 발행된 이벤트를 수신자에게 전송하는 구조로 수신자는 그 이벤트를 처리하는 방식으로, 상호 간 결합도를 낮추기 위해 비동기 방식으로 메시지를 전달하는 패턴이다.
<br /></p>

<h3 id="kafka의-기본-구성-요소">Kafka의 기본 구성 요소</h3>
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2dshY%2FbtqT6IEz5UC%2FhRRrU8AKeqTs98eUxbj3l0%2Fimg.png" alt="image" />
출처 : https://jyeonth.tistory.com/30</p>
<ol>
  <li><b>Cluster</b> : 여러 대의 컴퓨터들이 연결되어 하나의 시스템처럼 동작하는 컴퓨터들의 집합</li>
  <li><b>Producer</b> : 데이터를 만들어내어 전달하는 전달자의 역할</li>
  <li><b>Consumer</b> : 프로듀서에서 전달한 데이터를 브로커에 요청하여 메시지를 소비하는 역할</li>
  <li><b>Broker</b> : 생산자와 소비자와의 중재자 역할을 하는 역할</li>
  <li><b>Topic</b> : 보내는 메시지를 구분하기 위한 카테고리화</li>
  <li><b>Partition</b> : 토픽을 구성하는 데이터 저장소로서 수평확장이 가능한 형태</li>
</ol>

<p>Producer는 Topic에 메세지를 보내고, 하나의 Topic은 한 개 이상의 Partition으로 나눠진다. Partition은 Topic을 분할한 단위이며 Partition이 여러 개일 경우 Producer가 보낸 메세지의 순서는 보장할 수 없지만 Partition 안에서의 메세지는 순서가 보장된다.
<br /><br /><br /><br /></p>
<h2 id="이해를-위한-단순-예제">이해를 위한 단순 예제</h2>
<p>WebBrowser -&gt; Controller(producer) -&gt; Consumer 순으로 동작</p>

<pre><code class="language-Java">spring:
  kafka:
    bootstrap-servers:
    - localhost:9092
    consumer:
      group-id: testconsume
      auto-offset-reset: earliest
      key-deserializer:
        org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer:
        org.apache.kafka.common.serialization.StringDeserializer
    producer:
      bootstrap-servers:
      - localhost:9092
</code></pre>
<p>application.yml
<br /><br /><br /></p>

<pre><code class="language-Java">@RestController
@RequestMapping(value = "/test")
@RequiredArgsConstructor
public class TestController {
	
	private final KafkaTemplate&lt;String, String&gt; kafkaTemplate;
	
	private static String TOPIC_NAME = "test";
	
	@PostMapping
	public String sendMessage() {
		String messageData = "이거 돌아가는거 맞는건가";
		kafkaTemplate.send(TOPIC_NAME, messageData);
		return "성공이라고요";
	}
}
</code></pre>
<p>Postman을 통해 /test url에 요청을 보내면 KafkaTemplate를 통해 <b>test</b>라는 토픽에 이벤트를 발행합니다.
<br /><br /><br /></p>

<pre><code class="language-Java">@Service
public class TestConsumer {
	@KafkaListener(topics = "test")
    public void messageListener(ConsumerRecord&lt;String, String&gt; record) {
        log.info("### record: " + record.toString());
        log.info("### topic: " + record.topic() + ", value: " + record.value() + ", offset: " + record.offset());
	}
}
</code></pre>
<p><b>test</b>라는 Topic을 구독하고 있던 Consumer에서 데이터를 받고 출력합니다.<br /><br /><br /><br /></p>

<h3 id="정리-">정리 😵</h3>
<p>단순하게 생각해서 Producer에서 이벤트를 발행하면 Consumer에서 데이터 받아서 자기 할 일 한다고 생각하면 될 것 같습니다.<br />
예제가 간단해서 Kafka의 필요성을 잘 느끼시지 못할 것 같은데 <b>제가 생각한</b> Kafka의 장점은<br /></p>
<ol>
  <li>여러 서비스가 얽혀있는 복잡한 시스템에서 한 번의 이벤트 발행으로 여러 서비스를 실행시킬 수 있다.</li>
  <li>처리 속도가 빠르다.</li>
  <li>일관된 결과를 가져올 수 있다.</li>
  <li>처리 순서를 안정적으로 유지할 수 있다.</li>
</ol>

<p>더 많은 장점을 갖고 있겠지만 굉장히 복잡하다는 단점도 있을거라고 생각됩니다. 하지만 데이터를 빠르게 처리해야 하는 상황이라면 Kafka 사용을 고려하는 것이 좋을 것 같습니다.😁</p>]]></content><author><name>김혁준</name></author><category term="블로그" /><category term="blog" /><category term="kafka" /><summary type="html"><![CDATA[안녕하세요 F4팀의 혁잔디입니다. Apache Kafka를 활용한 프로젝트를 기획하면서 Apache Kakfa에 대해 제가 이해한 점을 정리하고 공유하기 위해 글을 작성합니다. 부족한 부분이 있으면 말씀해주셔요😊 Apache Kafka Apache Kafka는 웹사이트, 어플리케이션, 센서 등에 취합한 데이터를 스트림 파이프라인을 통해 실시간으로 관리하고 보내기 위한 분산 스트리밍 플랫폼입니다. 데이터를 생성하는 어플리케이션과 데이터를 소비하는 어플리케이션 간의 중재자 역할을 함으로써 데이터의 전송 제어, 처리, 관리 역할을 합니다. 카프카 시스템은 여러 요소(노드)와 함께 구성될 수 있어 카프카 클러스터 라고 하기도 하는데 다른 메시징 시스템과 마찬가지로 어플리케이션과 서버 간의 비동기 데이터 교환을 용이하게 하고, 하루에 수 조개의 이벤트 처리가 가능하게 하는 역할을 합니다. 즉, 카프카는 플랫폼에 서비스를 연결하여 다양한 서비스에서 나오는 데이터 흐름을 실시간으로 제어하는 서비스의 중추역할을 하는 플랫폼입니다. Kafka의 주요 기능 애플리케이션에서 데이터 또는 이벤트 스트림을 게시하거나 구독할 수 있게 합니다. 장애가 발생하더라도 안전하고 안정적인 방식으로 정확하게 레코드를 저장합니다. 레코드를 순서대로 실시간 처리합니다.]]></summary></entry><entry><title type="html">기술 블로그를 개설했습니다!</title><link href="http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/13/post01.html" rel="alternate" type="text/html" title="기술 블로그를 개설했습니다!" /><published>2023-08-13T00:00:00+09:00</published><updated>2023-08-13T00:00:00+09:00</updated><id>http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/13/post01</id><content type="html" xml:base="http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/13/post01.html"><![CDATA[<h2 id="-첫-포스팅">😀 첫 포스팅</h2>

<p>안녕하세요! 6팀의 PM 김지운입니다🧑🏻‍💻<br />
<br />
우리은행과 글로벌 소프트웨어 캠퍼스가 주관하는 “우리 FISA 1기 - 클라우드 엔지니어링”의 여섯 번째 팀. 팀 F4의 기술 블로그를 개설했습니다.<br />
<br />
저희 팀은 프론트엔드부터 인프라(퍼블릭 클라우드 &amp; 온프레미스)에 대한 지식을 학습하고 사용함으로써 개발의 A-Z 프로세스를 이해하려 노력합니다.<br />
<br />
저희의 지식 탐구에 대한 내용을 많은 사람들과 공유하고 논의하기 위해 블로그를 개설하였습니다. 추가적으로 이러한 학습 내용들이 파이널 프로젝트에는 어떻게 녹아들었는지 보여드리려 합니다. 주마다 올라올 저와 팀원들의 포스팅을 관심있게 지켜봐 주시면 감사하겠습니다!</p>

<h3 id="프로젝트-간단-소개">프로젝트 간단 소개</h3>

<p>저희팀은 서버 문제 발생으로 서비스들이 장애가 발생하는 원인을 해결하고자 하였고, 전자 상거래 서비스가 저희의 솔루션을 테스트 해보기 좋은 주제라고 생각했습니다. 따라서, “미술품 경매 거래 사이트”라는 주제로 “실시간 경매”, “미술품 구매”와 같은 두가지 핵심 기능을 구현하고, 서버 이중화와 서비스 사용량에 따른 인프라 관리 그리고 백엔드에서의 트래픽 분산 등을 적용해볼 계획입니다.</p>]]></content><author><name>김지운</name></author><category term="블로그" /><category term="blog" /><category term="project" /><summary type="html"><![CDATA[😀 첫 포스팅]]></summary></entry><entry><title type="html">ReplyingKafkaTemplate 예제 만들어보기</title><link href="http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/13/post02.html" rel="alternate" type="text/html" title="ReplyingKafkaTemplate 예제 만들어보기" /><published>2023-08-13T00:00:00+09:00</published><updated>2023-08-13T00:00:00+09:00</updated><id>http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/13/post02</id><content type="html" xml:base="http://localhost:4000/%EB%B8%94%EB%A1%9C%EA%B7%B8/2023/08/13/post02.html"><![CDATA[<p>안녕하세요 F4팀의 혁잔디입니다. Kafka를 활용한 프로젝트를 설계하면서 <b>Client의 요청에 대한 응답을 Kafka로 어떻게 처리할 수 있을까</b>에 대한 궁금증이 생겨 관련 자료를 찾아보았고 이해를 위해 간단한 예제를 만들어보았습니다.</p>

<h2 id="예제-간단-소개">예제 간단 소개</h2>

<p>Database에 User, Order 테이블 존재<br />
User 테이블에는 UserId와 Money(보유한 금액)이 존재<br />
Order 테이블에는 UserId, Paid(결제 금액), Result(결과) 존재<br />
User가 결제 금액을 요청했을 때 보유 금액이 결제 금액보다 크거나 같은 경우 “성공했습니다.” 반환받고<br />
그렇지 않은 경우 “실패했습니다.” 반환받음<br /></p>

<h2 id="apache-kafka를-사용한-동기식-요청-응답">Apache Kafka를 사용한 동기식 요청-응답</h2>

<p>Apache Kafka는 스트리밍 데이터와 다양한 생산자와 소비자 간의 디커플링을 위해 구축되었기 때문에 정말 필요한 상황이 아니면 Kafka와 요청-응답 개념을 사용하지 않는 것이 좋습니다.<br />
하지만 필요한 상황에서는 Spring kafka 템플릿을 사용하여 동기식 요청-응답을 구현할 수 있습니다.<br />
간단한 예제를 통해 사용 방법을 익혀보았습니다.<br />
<br />
<img src="https://github.com/wooriFisa-Final-Project-F4/wooriFisa-Final-Project-F4.github.io/assets/119636839/0352eced-733b-4f58-a3ab-2e7031c32613" alt="flow" />
<br />
PostMan을 통해 Controller에 JSON타입으로 요청을 보내면<br />
Controller에서 ProducerRecord를 통해 헤더에 Reply_Topic을 “order”로 설정해 주고 “user” topic에 이벤트 발행<br />
“user” topic을 구독하던 Consumer에서 간단한 로직 처리 후 @SendTo에 의해 헤더에 있는 Reply_Topic에 결과 반환<br />
Controller에서 ConsumerRecord를 통해 결과값 받은 후 Client에 반환</p>

<h3 id="controller">Controller</h3>

<pre><code class="language-Java">@PostMapping(value="/order",produces=MediaType.APPLICATION_JSON_VALUE,consumes=MediaType.APPLICATION_JSON_VALUE)
	public String sum(@RequestBody OrderEntity request) throws InterruptedException, ExecutionException {
		// create producer record
		ProducerRecord&lt;String, OrderEntity&gt; record = new ProducerRecord&lt;String, OrderEntity&gt;(requestTopic, request);
		// set reply topic in header
		record.headers().add(new RecordHeader(KafkaHeaders.REPLY_TOPIC, requestReplyTopic.getBytes()));
		// post in kafka topic
		RequestReplyFuture&lt;String, OrderEntity, OrderEntity&gt; sendAndReceive = kafkaTemplate.sendAndReceive(record);

		// confirm if producer produced successfully
		SendResult&lt;String, OrderEntity&gt; sendResult = sendAndReceive.getSendFuture().get();
		
		//print all headers
		sendResult.getProducerRecord().headers().forEach(header -&gt; System.out.println(header.key() + ":" + header.value().toString()));
		// get consumer record
		ConsumerRecord&lt;String, OrderEntity&gt; consumerRecord = sendAndReceive.get();
		
		// return consumer value
		return consumerRecord.value().getResult();
	}
</code></pre>

<h3 id="consumer">Consumer</h3>

<pre><code class="language-Java">@KafkaListener(topics = "${kafka.topic.request-topic}")
	@SendTo
	public OrderEntity listen(OrderEntity request) throws InterruptedException {
		if(uRepository.findById(request.getUserId()).isPresent()) {
			UserEntity user = uRepository.findById(request.getUserId()).get();
			if(user.getMoney() &lt; request.getPaid()) {
				request.setResult("Failure");
			}else {
				user.setMoney(user.getMoney()-request.getPaid());
				request.setResult("Success");
			}
			uRepository.save(user);
			oRepository.save(request);
			return request;
		}else {
			request.setResult("없는 사용자");
			return request;
		}
	}
</code></pre>

<p><br /><br /></p>
<h2 id="git">Git</h2>
<p>Configuration 등 설정이나 전체 코드는 <br />
https://github.com/rlagurnws/kafkatest.git v2.1 tag에서 확인 가능합니다.</p>]]></content><author><name>김혁준</name></author><category term="블로그" /><category term="blog" /><category term="kafka" /><summary type="html"><![CDATA[안녕하세요 F4팀의 혁잔디입니다. Kafka를 활용한 프로젝트를 설계하면서 Client의 요청에 대한 응답을 Kafka로 어떻게 처리할 수 있을까에 대한 궁금증이 생겨 관련 자료를 찾아보았고 이해를 위해 간단한 예제를 만들어보았습니다.]]></summary></entry></feed>